{
  "hash": "6acadc8b4504c071d253205622b96e4d",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Week 6\"\noutput: html_notebook\n---\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n:::\n\n\n\n# 1 - Deriving \"clustered\" standard errors\n\nRecall that if we have a random sample of $C$ \"clusters\" of observations of $X$ from $c=1,2,,,C$, each with $N_{c}$ observations, then the sample mean can be written as:\n\n$$ \\overline{X} = \\frac{1}{\\overline{N}}\\sum_{c=1}^{C}\\sum_{n=1}^{N_{c}} X_{n}.$$\nwhere $\\overline{N} = \\sum_{c}N_{c}$ is the total number of observations.\n\nSuppose that $\\mathbb{E}[X]=0$, and that observations of $X$ are independent across clusters but *not* within them. Then the variance of the sample mean is:\n\n$$ V[\\overline{X}] = \\frac{1}{\\overline{N}^2}\\times C \\times \\mathbb{E}\\left[\\left(\\sum_{n=1}^{N_{c}}X_{n}\\right)^2\\right] $$\n\nwhich we could then estimate by substituting in the sample mean.\n\nNow suppose instead that we have the regression model:\n\n$$ Y_{c,n} = X_{c,n}\\beta + \\epsilon_{c,n} $$\n\nwhere the error terms $\\epsilon_{c,n}$ are independent across clusters but not within them. In other words, $\\epsilon_{c,n}$ and $\\epsilon_{d,m}$ are independent only if $c\\neq d$. Recall that:\n\n$$ V[\\hat{\\beta}] = V[(\\mathbf{X}'\\mathbf{X})^{-1}\\mathbf{X}'{\\epsilon}]$$\n\nwhich we now must calculate allowing for the correlation with each cluster. Following the logic above, we can estimate the variance of $\\hat{\\beta}$ as:\n\n$$ \\widehat{V[\\hat{\\beta}]} = (\\mathbf{X}'\\mathbf{X})^{-1}\\sum_{c=1}^{C}\\left(\\sum_{n=1}^{N_{c}}X_{c,n}'\\hat{\\epsilon_{c,n}}\\right)\\left(\\sum_{n=1}^{N_{c}}\\hat{\\epsilon}_{n,c}X_{c,n}\\right)(\\mathbf{X}'\\mathbf{X})^{-1}$$\n\nwhich if we let $\\mathbf{X}_{c}$ be all the observations in cluster $c$ stacked in a matrix, and the same for $\\hat{\\epsilon}_{c}$, can be written as:\n\n$$ \\widehat{V[\\hat{\\beta}]} = (\\mathbf{X}'\\mathbf{X})^{-1}\\sum_{c=1}^{C}\\left(\\mathbf{X}_{c}'\\hat{\\epsilon}_{c}\\hat{\\epsilon}_{c}'\\mathbf{X}_{c}\\right)(\\mathbf{X}'\\mathbf{X})^{-1}.$$\n\nThis is the calculation being made when researchers refer to \"clustering\" their standard errors.\n\n# 2 - An Example\n\nSuppose that we are evaluating a classroom intervention and the model of outcomes for student $n$ in classroom $c$ for school $s$ is:\n\n$$ Y_{s,c,n} = \\alpha D_{s} + \\mu_{s} + \\gamma_{c} + \\epsilon_{n} $$\n\nWe will assume 3 classrooms per school and 10 students per classroom. Here, $\\alpha$ is the parameter of interest, the treatment effect of the intervention. Notice that $D_{s}$ is assigned at the level of the school.\n\nHere is code to generate the outcomes:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngenerate_data <- function(alpha,S) {\n  # draw the classroom effect\n  mu_s = rnorm(S)\n  D = runif(S)<0.5\n  d = data.frame()\n  for (c in 1:3) {\n    gamma_c = rnorm(S,0.2)\n    for (n in 1:10) {\n      eps_n = rnorm(S,0.2)\n      d = rbind(d,data.frame(school = 1:S,class = (1:S - 1)*3 + c,D = D,y = alpha*D + mu_s + gamma_c + eps_n))\n    }\n  }\n  d\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nd = generate_data(0.1,100)\n```\n:::\n\n\n\nQuestions:\n\n  (1) Notice that the error terms must be correlated. What is the appropriate level of clustering here?\n  (2) Previously we saw that including fixed effects could also remove correlation. What prohibits us from doing that in this model? (hint: would the rank condition hold if we used school or classroom fixed effects?)\n\nHere is a way to estimate the model with proper standard errors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(fixest)\n\nfeols(y ~ D,d) %>%\n  summary(cluster = \"school\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOLS estimation, Dep. Var.: y\nObservations: 3,000 \nStandard-errors: Clustered (school) \n            Estimate Std. Error  t value Pr(>|t|) \n(Intercept) 0.298715   0.185445 1.610804  0.11041 \nDTRUE       0.156478   0.240978 0.649347  0.51762 \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 1.74154   Adj. R2: 0.001678\n```\n\n\n:::\n:::\n\n\nWhich we can compare to standard errors without the clustering:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfeols(y ~ D,d) %>%\n  summary(\"standard\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nOLS estimation, Dep. Var.: y\nObservations: 3,000 \nStandard-errors: IID \n            Estimate Std. Error t value   Pr(>|t|)    \n(Intercept) 0.298715   0.045909 6.50671 8.9695e-11 ***\nDTRUE       0.156478   0.063664 2.45787 1.4033e-02 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nRMSE: 1.74154   Adj. R2: 0.001678\n```\n\n\n:::\n:::\n\n\n\nIn order to see the consequence of failing to deal with the correlation in errors, let's check the actual size of a two-sided test that $\\alpha=0$ when this is the true value. If the standard errors are correct, we should get close to 5\\% when we choose critical values of 1.96.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# a function to test that $\\alpha=0$ against a two-sided alternative\ntestcoef <- function(mod) {\n  stat <- mod$coefficients[2]/mod$se[2]\n  abs(stat)>1.96 \n}\n\n# tests the two assumptions for a sample of size S for B montecarlo trials\nsimulation_test <- function(B,S) {\n  test1 <- numeric(B)\n  test2 <- numeric(B)\n  for (b in 1:B) {\n    d = generate_data(0,S)\n    mod <- feols(y ~ D,d)\n    se1 = mod$se[2] #<- assuming iid erros\n    se2 = sqrt(vcov(mod,cluster =\"school\")[2,2])\n    t1 = mod$coefficients[2]/se1\n    t2 = mod$coefficients[2]/se2\n    test1[b] <- abs(t1)>1.96\n    test2[b] <- abs(t2)>1.96\n  }\n  print(paste(\"The mean rejection rate under the iid assumption is \",mean(test1)))\n  print(paste(\"The mean rejection rate when clustering standard errors by school is \",mean(test2)))\n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsimulation_test(100,100)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"The mean rejection rate under the iid assumption is  0.64\"\n[1] \"The mean rejection rate when clustering standard errors by school is  0.04\"\n```\n\n\n:::\n:::\n\n\nLook how badly the test does when the correlation in errors is not accounted for! In this case, it drastically overstates the precision that is available from these 3000 observations. The size of the clustered standard errors won't always be perfect, but it will get increasingly correct as $S\\rightarrow\\infty$.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}